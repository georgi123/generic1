2016-03-21 08:10:04,992 [main] INFO  org.apache.pig.Main - Apache Pig version 0.12.0-cdh5.1.0 (rexported) compiled Jul 12 2014, 08:41:12
2016-03-21 08:10:04,992 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/joro/generic-import-pl/import_engine_workaround/pig_1458544204990.log
2016-03-21 08:10:06,108 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found
2016-03-21 08:10:06,423 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2016-03-21 08:10:06,423 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2016-03-21 08:10:06,423 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://ipxx-apgxl01.gfk.com:8020
2016-03-21 08:10:07,060 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2016-03-21 08:10:07,364 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2016-03-21 08:10:07,399 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2016-03-21 08:10:07,502 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2016-03-21 08:10:07,598 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2016-03-21 08:10:08,638 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN
2016-03-21 08:10:08,709 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, DuplicateForEachColumnRewrite, GroupByConstParallelSetter, ImplicitSplitInserter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NewPartitionFilterOptimizer, PartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier]}
2016-03-21 08:10:08,757 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for Data: $0
2016-03-21 08:10:08,938 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false
2016-03-21 08:10:08,984 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1
2016-03-21 08:10:08,984 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1
2016-03-21 08:10:09,041 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - session.id is deprecated. Instead, use dfs.metrics.session-id
2016-03-21 08:10:09,042 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=
2016-03-21 08:10:09,067 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job
2016-03-21 08:10:09,251 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent
2016-03-21 08:10:09,251 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2016-03-21 08:10:09,251 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
2016-03-21 08:10:09,522 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job5595940613971869816.jar
2016-03-21 08:10:13,544 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job5595940613971869816.jar created
2016-03-21 08:10:13,545 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.jar is deprecated. Instead, use mapreduce.job.jar
2016-03-21 08:10:13,565 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2016-03-21 08:10:13,575 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.
2016-03-21 08:10:13,576 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cache
2016-03-21 08:10:13,576 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []
2016-03-21 08:10:13,665 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2016-03-21 08:10:13,666 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address
2016-03-21 08:10:13,676 [JobControl] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2016-03-21 08:10:13,699 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2016-03-21 08:10:14,066 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 24
2016-03-21 08:10:14,126 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 4
2016-03-21 08:10:14,144 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:4
2016-03-21 08:10:14,548 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local521298060_0001
2016-03-21 08:10:14,621 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/staging/root521298060/.staging/job_local521298060_0001/job.xml:an attempt to override final parameter: hadoop.ssl.require.client.cert;  Ignoring.
2016-03-21 08:10:14,622 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/staging/root521298060/.staging/job_local521298060_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
2016-03-21 08:10:14,623 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/staging/root521298060/.staging/job_local521298060_0001/job.xml:an attempt to override final parameter: hadoop.ssl.client.conf;  Ignoring.
2016-03-21 08:10:14,624 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/staging/root521298060/.staging/job_local521298060_0001/job.xml:an attempt to override final parameter: hadoop.ssl.keystores.factory.class;  Ignoring.
2016-03-21 08:10:14,626 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/staging/root521298060/.staging/job_local521298060_0001/job.xml:an attempt to override final parameter: hadoop.ssl.server.conf;  Ignoring.
2016-03-21 08:10:14,634 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/staging/root521298060/.staging/job_local521298060_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
2016-03-21 08:10:14,860 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1458544214731/elephant-bird-hadoop-compat-4.3.jar <- /home/joro/generic-import-pl/import_engine_workaround/elephant-bird-hadoop-compat-4.3.jar
2016-03-21 08:10:14,882 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized hdfs://ipxx-apgxl01.gfk.com:8020/tmp/temp-277138641/tmp2045915536/elephant-bird-hadoop-compat-4.3.jar as file:/tmp/hadoop-root/mapred/local/1458544214731/elephant-bird-hadoop-compat-4.3.jar
2016-03-21 08:10:14,993 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1458544214732/elephant-bird-core-4.3.jar <- /home/joro/generic-import-pl/import_engine_workaround/elephant-bird-core-4.3.jar
2016-03-21 08:10:14,997 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized hdfs://ipxx-apgxl01.gfk.com:8020/tmp/temp-277138641/tmp1911037962/elephant-bird-core-4.3.jar as file:/tmp/hadoop-root/mapred/local/1458544214732/elephant-bird-core-4.3.jar
2016-03-21 08:10:14,997 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1458544214733/elephant-bird-pig-4.3.jar <- /home/joro/generic-import-pl/import_engine_workaround/elephant-bird-pig-4.3.jar
2016-03-21 08:10:15,000 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized hdfs://ipxx-apgxl01.gfk.com:8020/tmp/temp-277138641/tmp23040287/elephant-bird-pig-4.3.jar as file:/tmp/hadoop-root/mapred/local/1458544214733/elephant-bird-pig-4.3.jar
2016-03-21 08:10:15,000 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1458544214734/json-simple-1.1.jar <- /home/joro/generic-import-pl/import_engine_workaround/json-simple-1.1.jar
2016-03-21 08:10:15,003 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized hdfs://ipxx-apgxl01.gfk.com:8020/tmp/temp-277138641/tmp-529344101/json-simple-1.1.jar as file:/tmp/hadoop-root/mapred/local/1458544214734/json-simple-1.1.jar
2016-03-21 08:10:15,084 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/local/localRunner/root/job_local521298060_0001/job_local521298060_0001.xml:an attempt to override final parameter: hadoop.ssl.require.client.cert;  Ignoring.
2016-03-21 08:10:15,084 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/local/localRunner/root/job_local521298060_0001/job_local521298060_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
2016-03-21 08:10:15,084 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/local/localRunner/root/job_local521298060_0001/job_local521298060_0001.xml:an attempt to override final parameter: hadoop.ssl.client.conf;  Ignoring.
2016-03-21 08:10:15,085 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/local/localRunner/root/job_local521298060_0001/job_local521298060_0001.xml:an attempt to override final parameter: hadoop.ssl.keystores.factory.class;  Ignoring.
2016-03-21 08:10:15,085 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/local/localRunner/root/job_local521298060_0001/job_local521298060_0001.xml:an attempt to override final parameter: hadoop.ssl.server.conf;  Ignoring.
2016-03-21 08:10:15,086 [JobControl] WARN  org.apache.hadoop.conf.Configuration - file:/tmp/hadoop-root/mapred/local/localRunner/root/job_local521298060_0001/job_local521298060_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
2016-03-21 08:10:15,087 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1458544214731/elephant-bird-hadoop-compat-4.3.jar
2016-03-21 08:10:15,087 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1458544214732/elephant-bird-core-4.3.jar
2016-03-21 08:10:15,087 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1458544214733/elephant-bird-pig-4.3.jar
2016-03-21 08:10:15,087 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1458544214734/json-simple-1.1.jar
2016-03-21 08:10:15,094 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/
2016-03-21 08:10:15,094 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local521298060_0001
2016-03-21 08:10:15,094 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases Data
2016-03-21 08:10:15,094 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: Data[8,7],Data[10,7],Data[12,7] C:  R: 
2016-03-21 08:10:15,096 [Thread-16] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null
2016-03-21 08:10:15,102 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2016-03-21 08:10:15,130 [Thread-16] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.jar is deprecated. Instead, use mapreduce.job.jar
2016-03-21 08:10:15,130 [Thread-16] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS
2016-03-21 08:10:15,131 [Thread-16] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent
2016-03-21 08:10:15,131 [Thread-16] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2016-03-21 08:10:15,146 [Thread-16] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter
2016-03-21 08:10:15,171 [Thread-16] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks
2016-03-21 08:10:15,172 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local521298060_0001_m_000000_0
2016-03-21 08:10:15,265 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]
2016-03-21 08:10:15,271 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :7
Total Length = 128092018
Input split[0]:
   Length = 21298959
  Locations:

-----------------------
Input split[1]:
   Length = 20354643
  Locations:

-----------------------
Input split[2]:
   Length = 20779430
  Locations:

-----------------------
Input split[3]:
   Length = 21250091
  Locations:

-----------------------
Input split[4]:
   Length = 15489250
  Locations:

-----------------------
Input split[5]:
   Length = 18041657
  Locations:

-----------------------
Input split[6]:
   Length = 10877988
  Locations:

-----------------------

2016-03-21 08:10:15,286 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed hdfs://ipxx-apgxl01.gfk.com:8020/user/hdfs/raw_ird/pl/20160311_20160319-2016032006/part-m-00009:0+21298959
2016-03-21 08:10:15,342 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new compressor [.snappy]
2016-03-21 08:10:15,547 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.zlib.ZlibFactory - Successfully loaded & initialized native-zlib library
2016-03-21 08:10:15,548 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.deflate]
2016-03-21 08:10:15,549 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.deflate]
2016-03-21 08:10:15,549 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.deflate]
2016-03-21 08:10:15,549 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.deflate]
2016-03-21 08:10:15,565 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.
2016-03-21 08:10:15,620 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map - Aliases being processed per job phase (AliasName[line,offset]): M: Data[8,7],Data[10,7],Data[12,7] C:  R: 
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
2016-03-21 08:10:19,948 [Service Thread] INFO  org.apache.pig.impl.util.SpillableMemoryManager - first memory handler call- Usage threshold init = 179306496(175104K) used = 127088280(124109K) committed = 179306496(175104K) max = 179306496(175104K)
2016-03-21 08:10:21,224 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:24,227 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:27,229 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:30,215 [Service Thread] INFO  org.apache.pig.impl.util.SpillableMemoryManager - first memory handler call - Collection threshold init = 179306496(175104K) used = 108627648(106081K) committed = 179306496(175104K) max = 179306496(175104K)
2016-03-21 08:10:30,232 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:33,233 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:36,235 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:39,236 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:40,646 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed hdfs://ipxx-apgxl01.gfk.com:8020/user/hdfs/raw_ird/pl/20160311_20160319-2016032006/part-m-00010:0+20354643
2016-03-21 08:10:42,238 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:45,240 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:48,242 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:51,244 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:54,245 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:10:57,246 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:00,249 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:03,251 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:06,266 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:06,451 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed hdfs://ipxx-apgxl01.gfk.com:8020/user/hdfs/raw_ird/pl/20160311_20160319-2016032006/part-m-00012:0+20779430
2016-03-21 08:11:09,267 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:12,269 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:15,434 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:18,750 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:21,927 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:25,008 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:28,010 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:31,075 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:34,077 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:37,079 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:40,081 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:43,248 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:46,370 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:49,372 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:52,374 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:55,375 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:11:58,391 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:01,436 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:04,368 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed hdfs://ipxx-apgxl01.gfk.com:8020/user/hdfs/raw_ird/pl/20160311_20160319-2016032006/part-m-00015:0+21250091
2016-03-21 08:12:04,561 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:07,562 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:10,607 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:13,666 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:16,667 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:19,668 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:22,690 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:25,691 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:28,723 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:31,758 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:32,888 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector@61f46f23
java.lang.IllegalArgumentException: You cannot call toBytes() more than once without calling reset()
	at parquet.Preconditions.checkArgument(Preconditions.java:47)
	at parquet.column.values.rle.RunLengthBitPackingHybridEncoder.toBytes(RunLengthBitPackingHybridEncoder.java:254)
	at parquet.column.values.rle.RunLengthBitPackingHybridValuesWriter.getBytes(RunLengthBitPackingHybridValuesWriter.java:63)
	at parquet.column.impl.ColumnWriterImpl.writePage(ColumnWriterImpl.java:170)
	at parquet.column.impl.ColumnWriterImpl.flush(ColumnWriterImpl.java:250)
	at parquet.column.impl.ColumnWriteStoreImpl.flush(ColumnWriteStoreImpl.java:105)
	at parquet.hadoop.InternalParquetRecordWriter.flushStore(InternalParquetRecordWriter.java:142)
	at parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:106)
	at parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:70)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.close(PigOutputFormat.java:149)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.close(MapTask.java:647)
	at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:1990)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:774)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2016-03-21 08:12:32,893 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local521298060_0001_m_000001_0
2016-03-21 08:12:32,947 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]
2016-03-21 08:12:32,953 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :6
Total Length = 123284304
Input split[0]:
   Length = 20611076
  Locations:

-----------------------
Input split[1]:
   Length = 20774177
  Locations:

-----------------------
Input split[2]:
   Length = 21379831
  Locations:

-----------------------
Input split[3]:
   Length = 20676083
  Locations:

-----------------------
Input split[4]:
   Length = 20037757
  Locations:

-----------------------
Input split[5]:
   Length = 19805380
  Locations:

-----------------------

2016-03-21 08:12:32,967 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed hdfs://ipxx-apgxl01.gfk.com:8020/user/hdfs/raw_ird/pl/20160311_20160319-2016032006/part-m-00000:0+20611076
2016-03-21 08:12:32,998 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new compressor [.snappy]
2016-03-21 08:12:33,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local521298060_0001_m_000002_0
2016-03-21 08:12:33,948 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]
2016-03-21 08:12:33,950 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :7
Total Length = 101193558
Input split[0]:
   Length = 21882509
  Locations:

-----------------------
Input split[1]:
   Length = 20938220
  Locations:

-----------------------
Input split[2]:
   Length = 20628550
  Locations:

-----------------------
Input split[3]:
   Length = 15443834
  Locations:

-----------------------
Input split[4]:
   Length = 10509778
  Locations:

-----------------------
Input split[5]:
   Length = 7092052
  Locations:

-----------------------
Input split[6]:
   Length = 4698615
  Locations:

-----------------------

2016-03-21 08:12:33,958 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed hdfs://ipxx-apgxl01.gfk.com:8020/user/hdfs/raw_ird/pl/20160311_20160319-2016032006/part-m-00003:0+21882509
2016-03-21 08:12:33,977 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new compressor [.snappy]
2016-03-21 08:12:34,244 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local521298060_0001_m_000003_0
2016-03-21 08:12:34,372 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]
2016-03-21 08:12:34,375 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :4
Total Length = 83320925
Input split[0]:
   Length = 21660980
  Locations:

-----------------------
Input split[1]:
   Length = 21539288
  Locations:

-----------------------
Input split[2]:
   Length = 20645278
  Locations:

-----------------------
Input split[3]:
   Length = 19475379
  Locations:

-----------------------

2016-03-21 08:12:34,382 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed hdfs://ipxx-apgxl01.gfk.com:8020/user/hdfs/raw_ird/pl/20160311_20160319-2016032006/part-m-00014:0+21660980
2016-03-21 08:12:34,398 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new compressor [.snappy]
2016-03-21 08:12:34,677 [Thread-16] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.
2016-03-21 08:12:34,792 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map
2016-03-21 08:12:34,799 [Thread-16] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local521298060_0001
java.lang.Exception: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at parquet.bytes.CapacityByteArrayOutputStream.addSlab(CapacityByteArrayOutputStream.java:97)
	at parquet.bytes.CapacityByteArrayOutputStream.write(CapacityByteArrayOutputStream.java:124)
	at java.io.ByteArrayOutputStream.writeTo(ByteArrayOutputStream.java:154)
	at parquet.bytes.BytesInput$BAOSBytesInput.writeAllTo(BytesInput.java:291)
	at parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:84)
	at parquet.column.impl.ColumnWriterImpl.writePage(ColumnWriterImpl.java:170)
	at parquet.column.impl.ColumnWriterImpl.accountForValueWritten(ColumnWriterImpl.java:159)
	at parquet.column.impl.ColumnWriterImpl.write(ColumnWriterImpl.java:217)
	at parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:254)
	at parquet.pig.TupleWriteSupport.writeValue(TupleWriteSupport.java:172)
	at parquet.pig.TupleWriteSupport.writeTuple(TupleWriteSupport.java:151)
	at parquet.pig.TupleWriteSupport.write(TupleWriteSupport.java:90)
	at parquet.pig.TupleWriteSupport.write(TupleWriteSupport.java:46)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:111)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:78)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:35)
	at parquet.pig.ParquetStorer.putNext(ParquetStorer.java:121)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:635)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:284)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
2016-03-21 08:12:36,397 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Ooops! Some job has failed! Specify -stop_on_failure if you want Pig to stop immediately on failure.
2016-03-21 08:12:36,397 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - job job_local521298060_0001 has failed! Stop running all dependent jobs
2016-03-21 08:12:36,398 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2016-03-21 08:12:36,399 [main] ERROR org.apache.pig.tools.pigstats.PigStatsUtil - 1 map reduce job(s) failed!
2016-03-21 08:12:36,404 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
2.3.0-cdh5.1.0	0.12.0-cdh5.1.0	root	2016-03-21 08:10:09	2016-03-21 08:12:36	UNKNOWN

Failed!

Failed Jobs:
JobId	Alias	Feature	Message	Outputs
job_local521298060_0001	Data	MAP_ONLY	Message: Job failed!	/user/root/parquet-pl/20160311,

Input(s):
Failed to read data from "/user/hdfs/raw_ird/pl/20160311_*/*"

Output(s):
Failed to produce result in "/user/root/parquet-pl/20160311"

Counters:
Total records written : 0
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_local521298060_0001


2016-03-21 08:12:36,404 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2016-03-21 08:12:36,424 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 2244: Job failed, hadoop does not return any error message
Details at logfile: /home/joro/generic-import-pl/import_engine_workaround/pig_1458544204990.log
2016-03-21 08:12:36,455 [Thread-47] WARN  org.apache.hadoop.hdfs.DFSClient - DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/root/parquet-pl/20160311/_temporary/0/_temporary/attempt_local521298060_0001_m_000003_0/part-m-00003.snappy.parquet (inode 53113473): File does not exist. Holder DFSClient_NONMAPREDUCE_-638462308_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
2016-03-21 08:12:36,457 [Thread-1] ERROR org.apache.hadoop.hdfs.DFSClient - Failed to close inode 53113473
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/root/parquet-pl/20160311/_temporary/0/_temporary/attempt_local521298060_0001_m_000003_0/part-m-00003.snappy.parquet (inode 53113473): File does not exist. Holder DFSClient_NONMAPREDUCE_-638462308_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
2016-03-21 08:12:36,462 [Thread-46] WARN  org.apache.hadoop.hdfs.DFSClient - DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/root/parquet-pl/20160311/_temporary/0/_temporary/attempt_local521298060_0001_m_000002_0/part-m-00002.snappy.parquet (inode 53113471): File does not exist. Holder DFSClient_NONMAPREDUCE_-638462308_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
2016-03-21 08:12:36,462 [Thread-1] ERROR org.apache.hadoop.hdfs.DFSClient - Failed to close inode 53113471
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/root/parquet-pl/20160311/_temporary/0/_temporary/attempt_local521298060_0001_m_000002_0/part-m-00002.snappy.parquet (inode 53113471): File does not exist. Holder DFSClient_NONMAPREDUCE_-638462308_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
2016-03-21 08:12:36,467 [Thread-45] WARN  org.apache.hadoop.hdfs.DFSClient - DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/root/parquet-pl/20160311/_temporary/0/_temporary/attempt_local521298060_0001_m_000001_0/part-m-00001.snappy.parquet (inode 53113469): File does not exist. Holder DFSClient_NONMAPREDUCE_-638462308_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
2016-03-21 08:12:36,468 [Thread-1] ERROR org.apache.hadoop.hdfs.DFSClient - Failed to close inode 53113469
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/root/parquet-pl/20160311/_temporary/0/_temporary/attempt_local521298060_0001_m_000001_0/part-m-00001.snappy.parquet (inode 53113469): File does not exist. Holder DFSClient_NONMAPREDUCE_-638462308_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
2016-03-21 08:12:36,472 [Thread-39] WARN  org.apache.hadoop.hdfs.DFSClient - DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/root/parquet-pl/20160311/_temporary/0/_temporary/attempt_local521298060_0001_m_000000_0/part-m-00000.snappy.parquet (inode 53113441): File does not exist. Holder DFSClient_NONMAPREDUCE_-638462308_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
2016-03-21 08:12:36,473 [Thread-1] ERROR org.apache.hadoop.hdfs.DFSClient - Failed to close inode 53113441
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/root/parquet-pl/20160311/_temporary/0/_temporary/attempt_local521298060_0001_m_000000_0/part-m-00000.snappy.parquet (inode 53113441): File does not exist. Holder DFSClient_NONMAPREDUCE_-638462308_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2991)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2796)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2680)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:362)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1438)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
